---
title: "Sup_Assignment2"
author: "Iggy Phiri"
date: "23 April 2019"
output: html_document
---

```{r}
setwd("C:/Users/Iggy.Phiri/Desktop/MPhi/Supervised learning/Assignment2")

```

```{r}
data <- read.csv("blocksTrain.csv", header = TRUE)
head(data)

#drop the ID col 

data <- data[,-1]
head(data)
View(data)

#scale of the data very different - therefore, perform MIN-MAX normalization (allows our data to fall in the range between 0 & 1)

data$height <- (data$height-min(data$height))/(max(data$height)-min(data$height))
data$length <- (data$length-min(data$length))/(max(data$length)-min(data$length))
data$area <- (data$area-min(data$area))/(max(data$area)-min(data$area))
data$eccen <- (data$eccen-min(data$eccen))/(max(data$eccen)-min(data$eccen))
data$p_black <- (data$p_black-min(data$p_black))/(max(data$p_black)-min(data$p_black))
data$p_and <- (data$p_and-min(data$p_and))/(max(data$p_and)-min(data$p_and))
data$mean_tr <- (data$mean_tr-min(data$mean_tr))/(max(data$mean_tr)-min(data$mean_tr))
data$blackpix <- (data$blackpix-min(data$blackpix))/(max(data$blackpix)-min(data$blackpix))
data$blackand <- (data$blackand-min(data$blackand))/(max(data$blackand)-min(data$blackand))
data$wb_trans <- (data$wb_trans-min(data$wb_trans))/(max(data$wb_trans)-min(data$wb_trans))
data$class <- (data$class-min(data$class))/(max(data$class)-min(data$class))

View(data)

write.csv(data, 'ann.csv')

```

```{r}
library(neuralnet)
library(caret)
```
```{r}
nnet_train <- data

#Discriminating between the class variable - COME BACK TO THIS - ggplot? 

qplot(nnet_train$class, data = nnet_train, colour = factor(nnet_train$class))

#Neural networks
set.seed(7)
nnet <- neuralnet(nnet_train$class ~.,data = nnet_train,
                  hidden = 2,
                  err.fct = "sse",
                  linear.output = FALSE,
                  threshold = 0.1)


plot(nnet)
 
nnet$weights
 
#Prediction 
 
nnpredict <- compute(nnet,nnet_train[,-11])
 
##predicted probability 
 
nnpredict_prob <- nnpredict$net.result
 
#comparing the predicted and actual values the model seems to be predicting well
head(nnpredict_prob)
head(nnet_train[,11])
 
 
 
 #Node output calculation with Sigmoid activation function - basically shows how the predicted values are calculated.
 
#Confusion matrix and misclassification error - Training data

predicted_value <- nnpredict_prob*(max(data1[,11])-min(data1[,11]))+min(data1[,11]) #converting the prediction back to it's origninal format 
actual_value <- data1[,11]

MSE <- sum((predicted_value-actual_value)^2)/nrow(data)
 
table(actual_value,round(predicted_value))


###################

Pred <- round(predicted_value)

tab1 <- table(actual_value,Pred)


#Misclassification Rate
Misclassification <- 1-((4242+233+5+29+37)/4925)


#Trying to improve the model performance by adjusting some parameters

set.seed(7)
nnet1 <- neuralnet(nnet_train$class ~.,data = nnet_train,
                  hidden = c(3,2),
                  err.fct = "sse",
                  linear.output = FALSE,
                  threshold = 0.1,
                  lifesign = "full",
                  rep = 5)

plot(nnet1)


nnet3 <- neuralnet(nnet_train$class ~.,data = nnet_train,
                  hidden = c(5,2),
                  err.fct = "sse",
                  linear.output = FALSE,
                  threshold = 0.1,
                  lifesign = "full",
                  rep = 5)



nnet4 <- neuralnet(nnet_train$class ~.,data = nnet_train,
                  hidden = c(6,1),
                  err.fct = "sse",
                  linear.output = FALSE,
                  threshold = 0.1,
                  lifesign = "full",
                  rep = 5)

nnet5 <- neuralnet(nnet_train$class ~.,data = nnet_train,
                  hidden = c(1,1),
                  err.fct = "sse",
                  linear.output = FALSE,
                  threshold = 0.1,
                  lifesign = "full",
                  rep = 5)

nnet6 <- neuralnet(nnet_train$class ~.,data = nnet_train,
                  hidden = c(3,1),
                  err.fct = "sse",
                  linear.output = FALSE,
                  threshold = 0.1,
                  lifesign = "full",
                  rep = 5)


nnet7 <- neuralnet(nnet_train$class ~.,data = nnet_train,
                  hidden = c(2,1),
                  err.fct = "sse",
                  linear.output = FALSE,
                  threshold = 0.1,
                  lifesign = "full",
                  rep = 5)

nnet8 <- neuralnet(nnet_train$class ~.,data = nnet_train,
                  hidden = c(3,2),
                  err.fct = "sse",
                  linear.output = FALSE,
                  threshold = 0.1,
                  lifesign = "full",
                  rep = 5)


```

BUILDING NEURAL TRAINING MODEL
```{r}
#Encoding a new dataset 
datax <- cbind(data[,1:10],class2ind(as.factor(data$class)))



#set label names for the data
names(datax) <- c(names(data2)[1:10],"txt","hor_line","pic", "ver_line", "graphic")

data2 <- datax[sample(nrow(datax)),]

####Split dat into training and test set: 80% training & 20% test 


set.seed(7)

size <- floor(0.8*nrow(datax))
train_data <- data2[1:size,]
test_data <- data2[c(size+1):nrow(datax),] 

#Training Model:

nn_formula <- txt+hor_line+pic+ver_line+graphic ~ height+length+area+eccen+p_black+p_and+mean_tr+blackpix+blackand+wb_trans

training_model <- train(form=nn_formula,
                        data=train_data,
                        method = "neuralnet",
                        tuneGrid = expand.grid(.layer1=c(1:4), .layer2=c(0:4), .layer3=c(0)),
                        learningrate = 0.01,
                        threshold = 0.01,
                        stepmax = 50000)

plot_trainmodel <- plot(training_model)

####Building Best Model

best_model <- neuralnet(formula = nn_formula, 
                 data = train_data,
                 hidden = c(1,1),                               #put hidden layer determined by the training set   
                 learningrate = 0.01,
                 err.fct = "sse",
                 threshold = 0.01,   
                 stepmax = 50000                   #consider reducing this ?
                 
)

plot(best_model, rep = 'best')


#### Measuring Model Performance on unseen train data 

train_prediction_prob <- compute(best_model,train_data[,1:10])

predicted_value1 <- train_prediction_prob$net.result

train_actual_value1 <- train_data[,11:15]



################### Creating classification table on unseen train data 

train_pred <- round(train_prediction_prob$net.result)
pred1 <- round(train_prediction_prob$net.result)
Train_Prediction_Results<- ifelse(pred1[,1]%in%1, "txt",
                               ifelse(pred1[,2]%in%1,"hor_line",
                                      ifelse(pred1[,3]%in%1,"pic",
                                             ifelse(pred1[,4]%in%1,"ver_line","graphic"))))





Train_Class <- ifelse(train_actual_value1[,1]%in%1, "txt",
                               ifelse(train_actual_value1[,2]%in%1,"hor_line",
                                      ifelse(train_actual_value1[,3]%in%1,"pic",
                                             ifelse(train_actual_value1[,4]%in%1,"ver_line","graphic"))))


####Confusion Matrix 

train_conf_matrix <- table(Train_Prediction_Results,Train_Class)


#####Training Misclassification Rate######


NN_train_misclassification <- 1-((198+3457)/3940)




#### Measuring Model Performance on unseen train data 

prediction_prob <- compute(best_model,test_data[,1:10])

predicted_value1 <- prediction_prob$net.result

actual_value1 <- test_data[,11:15]

#actual_value2 <- data.matrix(actual_value1)

MSE1 <- sum((predicted_value1-actual_value1)^2)/nrow(test_data)

Pred1 <- round(predicted_value1)

tab2 <- table(actual_value,Pred1) 

#table(actual_value,round(predicted_value))

#predicted_final <- predicted_value1
#predicted_final <- as.data.frame(predicted_final)



################### Creating classification table on unseen train data 

pred <- round(prediction_prob$net.result)

Prediction_Results<- ifelse(pred[,1]%in%1, "txt",
                               ifelse(pred[,2]%in%1,"hor_line",
                                      ifelse(pred[,3]%in%1,"pic",
                                             ifelse(pred[,4]%in%1,"ver_line","graphic"))))





Class <- ifelse(actual_value1[,1]%in%1, "txt",
                               ifelse(actual_value1[,2]%in%1,"hor_line",
                                      ifelse(actual_value1[,3]%in%1,"pic",
                                             ifelse(actual_value1[,4]%in%1,"ver_line","graphic"))))


####Confusion Matrix 

conf_matrix <- table(Prediction_Results,Class)


##Misclassification Rate


Misclassification1 <- 1-((56 +868)/985)


#########Unlabeled data prediction using NN########



nn_prediction_prob <- compute(best_model,undata[,2:11])

nn_predicted_value <- nn_prediction_prob$net.result

NN_Prediction <- round(nn_predicted_value)

NN_Prediction1 <- matrix(NN_Prediction,nrow = 548)

NN_Prediction1 <- cbind(undata[,1],NN_Prediction1)





colnames(NN_Prediction1)  <- c("ID","txt","hor_line","pic","ver_line","graphic" )

write.csv(NN_Prediction1, 'NN_Prediction.csv')




#########END OF PART 1###########


```

SVM

```{r}
#libraries 
library(e1071)
library(kernlab)
library(ggplot2)

data <- read.csv("blocksTrain.csv", header = TRUE)
head(data)

#drop the ID col 

data <- data[,-1]
head(data)

#MIN-MAX Normalization 

data$height <- (data$height-min(data$height))/(max(data$height)-min(data$height))
data$length <- (data$length-min(data$length))/(max(data$length)-min(data$length))
data$area <- (data$area-min(data$area))/(max(data$area)-min(data$area))
data$eccen <- (data$eccen-min(data$eccen))/(max(data$eccen)-min(data$eccen))
data$p_black <- (data$p_black-min(data$p_black))/(max(data$p_black)-min(data$p_black))
data$p_and <- (data$p_and-min(data$p_and))/(max(data$p_and)-min(data$p_and))
data$mean_tr <- (data$mean_tr-min(data$mean_tr))/(max(data$mean_tr)-min(data$mean_tr))
data$blackpix <- (data$blackpix-min(data$blackpix))/(max(data$blackpix)-min(data$blackpix))
data$blackand <- (data$blackand-min(data$blackand))/(max(data$blackand)-min(data$blackand))
data$wb_trans <- (data$wb_trans-min(data$wb_trans))/(max(data$wb_trans)-min(data$wb_trans))


head(data)

write.csv(data, 'svm.csv')

svm_data <- data
svm_data$class <- as.factor(svm_data$class)

#####splitting the data into training and test ######

set.seed(7)
data_svm <- svm_data[sample(nrow(svm_data)),]
size1 <- floor(0.8*nrow(svm_data))
train_data_svm <- data_svm[1:size1,]
test_data_svm <- data_svm[c(size1+1):nrow(svm_data),] 



```

```{r}
#####Plot of the data ######

f <- ggplot(train_data_svm, aes(wb_trans, blackand, colour = class))
f + geom_jitter() + scale_colour_hue() + theme(legend.position="bottom")


f1 <- ggplot(train_data_svm, aes(height, length, colour = class))
f1 + geom_jitter() + scale_colour_hue() + theme(legend.position="bottom")

f2 <- ggplot(train_data_svm, aes(length, height, colour = class))
f2 + geom_jitter() + scale_colour_hue() + theme(legend.position="bottom")


```
```{r}
##########SVM########
svm_model <- svm(class~.,data=train_data_svm,scale=FALSE,kernel="radial",cost=5)
svm_model
summary(svm_model) #number of vectors is 752, 328 belongs to 1 etc
svm_model$index

plot(svm_model,data =train_data_svm,
     height~length,
     slice=list(area=0.25, eccen=0.25, p_black=0.25, p_and=0.25, mean_tr=0.25, blackpix=0.25,blackand=0.25, wb_trans=0.25))


```
```{r}
########Confusiom Matrix & Misclassification Error#######

pred_svm <- predict(svm_model,train_data_svm)

tab_svm <- table(Predicted=pred_svm,Actual=train_data_svm$class)


svm_misclassification <- 1-sum(diag(tab_svm))/sum(tab_svm)   


################Using polynomial kernel 

pred_svm2 <- predict(svm_model2,train_data_svm)

tab_svm2 <- table(Predicted=pred_svm2,Actual=train_data_svm$class)


svm_misclassification2 <- 1-sum(diag(tab_svm2))/sum(tab_svm2)

###we see that misclassification increases when the poly kernel is used SAME effect when signoid is used 


##########Tuning model to obtain better classification ###################

set.seed(7)

tune_model <- tune(svm,class~.,data = train_data_svm,
     ranges = list(epsilon=seq(0,1,0.1),cost=2^(2:7)))                    #cost captures the cost of constrain violation - if the cost is too high it implies high penalty for non-seperable points. Thus the model would store too many support vectors leading to over fitting 


plot(tune_model)

summary(tune_model)

######Best Model ######

best_svm_model <- tune_model$best.model

summary(best_svm_model)




plot(best_svm_model,data =train_data_svm,
     height~length,
     slice=list(area=0.25, eccen=0.25, p_black=0.25, p_and=0.25, mean_tr=0.25, blackpix=0.25,blackand=0.25, wb_trans=0.25))




#########Confusion Matrix and Misclassification Error From Best Model

best_pred_svm <- predict(best_svm_model,train_data_svm)

tab_best_svm <- table(Predicted=best_pred_svm,Actual=train_data_svm$class)


best_svm_misclassification <- 1-sum(diag(tab_best_svm))/sum(tab_best_svm)   


######testing model performance on the test data ########


test_pred_svm <- predict(best_svm_model,test_data_svm)

tab_test_svm <- table(Predicted=test_pred_svm,Actual=test_data_svm$class)


test_svm_misclassification <- 1-sum(diag(tab_test_svm))/sum(tab_test_svm)


#######Using the provided unlabeled data to predict class SVM #####

undata <- read.csv("blocksTestNoLabel.csv",header = TRUE)


# MIN-MAX Normalization 

scl <- function(x){ (x - min(x))/(max(x) - min(x)) }
undata[, 2:11] <- data.frame(lapply(undata[, 2:11], scl))
head(undata)

######Predicting class on unseen data ######

Predicted_SVM_Value <- predict(best_svm_model,undata)

Tab_Predicted_SVM_Value <- table(Predicted=Predicted_SVM_Value)


SVM_Prediction <- matrix(Predicted_SVM_Value)
ID_matrix <- undata[,1]

SVM_Prediction <- cbind(ID_matrix,SVM_Prediction)
head(SVM_Prediction)

colnames(SVM_Prediction)  <- c("ID","class")

write.csv(SVM_Prediction, 'SVM_class_Prediction.csv')


```


